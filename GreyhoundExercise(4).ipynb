{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Finishing Times of Greyhounds in UK Races\n",
        "\n",
        "The goal of this project is to predict the finishing time of dogs in UK greyhound races based on data from their previous races. The objective is to minimize the mean squared error on our prediction data."
      ],
      "metadata": {
        "id": "VhYaF9mFhJYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "XPY47EWW0FJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "E6RFdJxh6UIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "YuwmYUYDRULA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Data"
      ],
      "metadata": {
        "id": "WTb8jMvTzoya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1JUiCZuzkOD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"df.csv\")\n",
        "unseendf = pd.read_csv(\"unseendf_example.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "NWrPulccn1Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "7ke2KStgnB_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the number of null values in each column\n",
        "num_null_count = df.isnull().sum()\n",
        "print(num_null_count)"
      ],
      "metadata": {
        "id": "JaKYFkya0puz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we can recognize that there are 158 rows without comments."
      ],
      "metadata": {
        "id": "xgKf7HC91fmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['birthdate'] = pd.to_datetime(df['birthdate'])\n",
        "df['date1'] = pd.to_datetime(df['date1'])\n",
        "df['date2'] = pd.to_datetime(df['date2'])\n",
        "df['days_since_last_race'] = (df['date2'] - df['date1']).dt.days # we need to add the .dt.days because we need to convert the time-delta objects to integers\n",
        "display(df['days_since_last_race'])"
      ],
      "metadata": {
        "id": "K6oK-GBr0tb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully added another column that measures the time since the last race. This is especiallly important because we want to determine if a dog has had ample rest before the following race."
      ],
      "metadata": {
        "id": "B1YWG5h6-_5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['speed'] = df['distance1'] / df['time1'] # Adding a speed column\n",
        "df['distance_ratio'] = df['distance2'] / df['distance1']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9yHIucdH--Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Categorical to Numerical Variables\n",
        "# One-hot encoding for 'stadium'\n",
        "df = pd.get_dummies(df, columns=['stadium'], drop_first=True)\n",
        "\n",
        "# Check the result\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tqGn8knnBBJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert the trap values to numerical data, by utillizing the Orginal Encoder. This also preserver this order and will be used well in our Random Forest Model."
      ],
      "metadata": {
        "id": "VIk_gf3QpTkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OrdinalEncoder()\n",
        "df[['trap1', 'trap2']] = encoder.fit_transform(df[['trap1', 'trap2']])"
      ],
      "metadata": {
        "id": "OQs72DWQtKsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following that, we no longer need the columns for the birthdate, and the dates between the two races."
      ],
      "metadata": {
        "id": "k_3du-1sqPhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['birthdate', 'date1', 'date2'])"
      ],
      "metadata": {
        "id": "rZOmGC_PuWdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['comment1'] = df['comment1'].fillna(\"No comment\")"
      ],
      "metadata": {
        "id": "bczQ6TWdETZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to use Sentiment Analysis to identify the emotions behind the text. This model uses a RoBERTa Model. Since we have many datapoints (~533,000), we take a random sample of 10% to use in our model, which is still a lot of data that we can use to generate our model."
      ],
      "metadata": {
        "id": "ACTy6xh-Mf1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "vCuXuk6W82BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can import the transformers library into the notebook, which provides RoBERTa, a very strong natural language processing model for sentiment analysis."
      ],
      "metadata": {
        "id": "YlDm1SIv9pfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "XKtBFK1jEwL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you pass the tokenized data through the RoBERTa model to extract embeddings. RoBERTaâ€™s output consists of:\n",
        "\n",
        "    last_hidden_state: Contains embeddings for all tokens in the sequence.\n",
        "    pooler_output: A summary embedding for the [CLS] token."
      ],
      "metadata": {
        "id": "Zc2n0u2LzVLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP on the Comments\n"
      ],
      "metadata": {
        "id": "-xExu1iX-QMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_comments_in_batches(comments, batch_size=16, max_length=128):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(comments), batch_size):\n",
        "        batch = comments[i:i + batch_size]\n",
        "        tokens = tokenizer(\n",
        "            batch.tolist(),\n",
        "            max_length=max_length,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokens = {key: value.to(device) for key, value in tokens.items()}  # Move tokens to our device\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**tokens)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :]  # Extracting the CLS token, which summarizes the entire comment\n",
        "        embeddings.append(batch_embeddings) # Adding that token to our embeddings\n",
        "\n",
        "    return torch.vstack(embeddings).cpu().numpy()  # Stacking all embeddings into a single tensor, them moving it to the CPU and then a numpy array\n",
        "\n",
        "# Using our Function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Process the comments in batches and save embeddings\n",
        "comment_embeddings = process_comments_in_batches(df_sample['comment1'], batch_size=16)\n"
      ],
      "metadata": {
        "id": "R7UpyPIBxvEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create a DataFrame for the Embeddings. The comment_embeddings variable is a NumPy array containing the embeddings extracted from the comments. Each row represents a comment, and each column corresponds to a specific dimension of the embedding (768 dimensions of RoBERTa).\n"
      ],
      "metadata": {
        "id": "Q9niKAC9J_ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for the embeddings\n",
        "embedding_columns = [f'comment_embedding_{i}' for i in range(comment_embeddings.shape[1])]\n",
        "embeddings_df = pd.DataFrame(comment_embeddings, columns=embedding_columns)\n",
        "\n",
        "# Reset index of the DataFrame to align with embeddings\n",
        "df_sample = df_sample.reset_index(drop=True)\n",
        "\n",
        "# Concatenating our Embeddings to our Originak DataFrame\n",
        "df_sample = pd.concat([df_sample, embeddings_df], axis=1)"
      ],
      "metadata": {
        "id": "xR5_O_ZvNFmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = df_sample.drop(columns=['time2', 'comment1'])\n",
        "y = df_sample['time2']  # Our Target variable\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "eqnNgWOZNHQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape: {X_train.shape}\")  # Rows and features\n",
        "print(f\"y_train shape: {y_train.shape}\")  # Target size"
      ],
      "metadata": {
        "id": "ZiM__Iqw23kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA\n",
        "pca = PCA(n_components=100, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Train the Random Forest Regressor on PCA-reduced data\n",
        "regression_model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
        "regression_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Make predictions on the PCA-reduced test data\n",
        "y_pred = regression_model.predict(X_test_pca)\n",
        "\n",
        "# Compare Actual vs. Predicted\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Actual': y_test.reset_index(drop=True),  # Reset index for alignment\n",
        "    'Predicted': y_pred\n",
        "})\n",
        "\n",
        "print(comparison_df.head())\n"
      ],
      "metadata": {
        "id": "gE0qcCnBNsPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = regression_model.predict(X_test_pca)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Optional: Calculate and print Root Mean Squared Error (RMSE)\n",
        "rmse = mse ** 0.5\n",
        "print(f\"Root Mean Squared Error: {rmse}\")"
      ],
      "metadata": {
        "id": "5634pJvI9kkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['time2'].var())"
      ],
      "metadata": {
        "id": "FS36CYAR9sq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Becuase the Varience is much greater than the mean squared error, we can conclude that our model is fairly accurate."
      ],
      "metadata": {
        "id": "wJ_TX_XVQHwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with the Unseen Dataset for Races"
      ],
      "metadata": {
        "id": "ZGmSfgwgQDYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load unseen data\n",
        "unseendf = pd.read_csv(\"unseendf.csv\")\n",
        "\n",
        "# Preprocess unseen data\n",
        "unseendf['date1'] = pd.to_datetime(unseendf['date1'])\n",
        "unseendf['date2'] = pd.to_datetime(unseendf['date2'])\n",
        "unseendf['days_since_last_race'] = (unseendf['date2'] - unseendf['date1']).dt.days\n",
        "unseendf['speed'] = unseendf['distance1'] / unseendf['time1']\n",
        "unseendf['distance_ratio'] = unseendf['distance2'] / unseendf['distance1']\n",
        "\n",
        "# One-hot encode the 'stadium' column\n",
        "unseendf = pd.get_dummies(unseendf, columns=['stadium'], drop_first=True)\n",
        "\n",
        "# Ensure all expected stadium columns are present\n",
        "expected_stadium_columns = [\n",
        "    'stadium_Crayford', 'stadium_Doncaster', 'stadium_Harlow',\n",
        "       'stadium_Henlow', 'stadium_Hove', 'stadium_Kinsley', 'stadium_Monmore',\n",
        "       'stadium_Newcastle', 'stadium_Nottingham', 'stadium_Oxford',\n",
        "       'stadium_Pelaw Grange', 'stadium_Perry Barr', 'stadium_Romford',\n",
        "       'stadium_Sheffield', 'stadium_Suffolk Downs', 'stadium_Sunderland',\n",
        "       'stadium_Swindon', 'stadium_Towcester', 'stadium_Yarmouth' # Include all stadiums seen during training\n",
        "]\n",
        "\n",
        "for col in expected_stadium_columns:\n",
        "    if col not in unseendf.columns:\n",
        "        unseendf[col] = 0\n",
        "\n",
        "# Align the columns with training features\n",
        "train_columns = [\n",
        "    \"days_since_last_race\", \"speed\", \"distance_ratio\",\n",
        "    'stadium_Crayford', 'stadium_Doncaster', 'stadium_Harlow',\n",
        "       'stadium_Henlow', 'stadium_Hove', 'stadium_Kinsley', 'stadium_Monmore',\n",
        "       'stadium_Newcastle', 'stadium_Nottingham', 'stadium_Oxford',\n",
        "       'stadium_Pelaw Grange', 'stadium_Perry Barr', 'stadium_Romford',\n",
        "       'stadium_Sheffield', 'stadium_Suffolk Downs', 'stadium_Sunderland',\n",
        "       'stadium_Swindon', 'stadium_Towcester', 'stadium_Yarmouth',  # All one-hot encoded stadium columns\n",
        "    \"trap1\", \"trap2\",\n",
        "    *(f\"comment_embedding_{i}\" for i in range(768))  # Adjust for embedding size\n",
        "]\n",
        "\n",
        "# Reindex unseen dataset to match training feature set\n",
        "unseendf = unseendf.reindex(columns=train_columns, fill_value=0)\n",
        "\n",
        "# Apply PCA and predict\n",
        "X_unseen_pca = pca.transform(unseendf)\n",
        "unseendf['predtime'] = regression_model.predict(X_unseen_pca)\n",
        "\n",
        "# Save predictions\n",
        "unseendf.to_csv(\"mypred.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "3UjW-Tao-0RM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}